{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining and Analysis Course Project\n",
    "\n",
    "\n",
    "# Course Code: 18ECSC301\n",
    "\n",
    "\n",
    "# Team: 5A09\n",
    "\n",
    "# Roll No : 39- Apoorva S Malemath - 01FE16BCS041\n",
    "\n",
    "# Roll No : 44- Arundati Dixit            - 01FE16BCS046\n",
    "\n",
    "# Roll No : 45- Ashish Kar                 - 01FE16BCS047\n",
    "\n",
    "# Roll No : 58- Deepti Nadkarni         - 01FE16BCS062\n",
    "![](weibo.jpg)\n",
    "\n",
    "# Project ID : 5ADMACP14  Sina Weibo Interaction Prediction Challenge\n",
    "# Predict the forwarding, commenting and liking amount of a Weibo posted by a user based on the historical interaction data on Sina Weibo social platform.\n",
    "\n",
    "## Determining Statistical Factors\n",
    "### Authors: Apoorva Malemath, Arundati Dixit, Ashish Kar, Deepti Nadkarni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------Understanding The Data----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Dataset Analysis                                                             \n",
    "# Contribution: All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "%pylab inline\n",
    "from googletrans import Translator\n",
    "header = ['u_id','m_id','time','content']\n",
    "predict_dataset= pd.read_fwf(\"G://DMA_PROJECT//weibo_predict_data.txt\",header=None,names=header,\n",
    "                     encoding='utf-8',delimiter=\"\\t\")\n",
    "# fixed width formatted lines.\n",
    "pedict_dataset.head(5)\n",
    "translate_dataframe = pd.DataFrame(data=predict_dataset['content'].head(30))\n",
    "translator = Translator()\n",
    "translate_dataframe[\"English_content\"] = translate_dataframe['content'].map(lambda x: translator.translate(x, src=\"zh-CN\", dest=\"en\").text)\n",
    "print(translate_dataframe)\n",
    "print(\"Predict_Dataset has \"+str(predict_dataset.shape[0])+\" records\")\n",
    "print(\"Predict_Dataset has \"+str(predict_dataset.shape[1])+\" attributes\")\n",
    "predict_dataset2=pd.DataFrame(predict_dataset.time.str.split(' ',1).tolist(),columns=['date','new_time'])\n",
    "predict_dataset3 = pd.concat([predict_dataset,predict_dataset2], axis=1)\n",
    "del predict_dataset3['time']\n",
    "predict_dataset3.rename(columns={'new_time':'time'},inplace=True)\n",
    "predict_dataset3.head(5)                                            \n",
    "predict_dataset3.to_csv(\"G://DMA_PROJECT//weibo_predict.csv\",sep=',',index=False,encoding='utf-8')                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis of Train Dataset\n",
    " Contribution: All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%pylab inline\n",
    "import copy\n",
    "from googletrans import Translator\n",
    "import emoji\n",
    "import re\n",
    "Names = ['u_id','m_id','time','content']\n",
    "train_dataset= pd.read_fwf(\"G://DMA_PROJECT//weibo_train_data.txt\",header=None,names=Names ,encoding='utf-8',delimiter=\"\\t\")\n",
    "# fixed width formatted lines.\n",
    "train_dataset.head(10)\n",
    "print(\"Predict_Dataset has \"+str(train_dataset.shape[0])+\" records\")\n",
    "print(\"Predict_Dataset has \"+str(train_dataset.shape[1])+\" attributes\")\n",
    "train_content_mix=train_dataset['content']\n",
    "train_content_split = pd.DataFrame(train_content_mix.str.split('\\t',expand=True))\n",
    "print(train_content_split.head(5))\n",
    "train_dataset2 = pd.concat([train_dataset,train_content_split], axis=1)\n",
    "print(train_dataset2.head(5))\n",
    "del train_dataset2['content']\n",
    "train_dataset2.rename(columns={0:'forward_count'},inplace=True)\n",
    "train_dataset2.rename(columns={1:'comment_count'},inplace=True)\n",
    "train_dataset2.rename(columns={2:'like_count'},inplace=True)\n",
    "train_dataset2.rename(columns={3:'content'},inplace=True)\n",
    "train_dataset2.head(5)\n",
    "translate_dataframe = pd.DataFrame(data=train_dataset2['content'].head(10))\n",
    "translator = Translator()\n",
    "translate_dataframe[\"English_content\"] = translate_dataframe['content'].map(lambda x: translator.translate(x, src=\"zh-CN\", dest=\"en\").text)\n",
    "print(translate_dataframe)\n",
    "\n",
    "train_dataset2.head(30)\n",
    "train_dataset2.forward_count.describe()\n",
    "train_dataset2.like_count.describe()\n",
    "\n",
    "train_dataset3=pd.DataFrame(train_dataset2.time.str.split(' ',1).tolist(),columns=['date','new_time'])\n",
    "train_dataset3.head()\n",
    "train_dataset4 = pd.concat([train_dataset2,train_dataset3], axis=1)\n",
    "del train_dataset4['time']\n",
    "train_dataset4.rename(columns={0:'forward_count'},inplace=True)\n",
    "train_dataset4.rename(columns={1:'comment_count'},inplace=True)\n",
    "train_dataset4.rename(columns={2:'like_count'},inplace=True)\n",
    "train_dataset4.rename(columns={'new_time':'time'},inplace=True)\n",
    "train_dataset4.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Month vs Like_Count\n",
    "train_dataset5=train_dataset4.sort_values('date',ascending=True)\n",
    "train_dataset5['like_count']=train_dataset5['like_count'].astype(float)\n",
    "train_dataset5['month']=pd.DatetimeIndex(train_dataset5['date']).month\n",
    "plt.plot(train_dataset5['month'],train_dataset5['like_count'])\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Month vs Forward_Count\n",
    "train_dataset5=train_dataset4.sort_values('date',ascending=True)\n",
    "train_dataset5['forward_count']=train_dataset5['forward_count'].astype(float)\n",
    "train_dataset5['month']=pd.DatetimeIndex(train_dataset5['date']).month\n",
    "plt.plot(train_dataset5['month'],train_dataset5['forward_count'])\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Month vs Comment_Count\n",
    "train_dataset5=train_dataset4.sort_values('date',ascending=True)\n",
    "train_dataset5['comment_count']=train_dataset5['comment_count'].astype(float)\n",
    "train_dataset5['month']=pd.DatetimeIndex(train_dataset5['date']).month\n",
    "plt.plot(train_dataset5['month'],train_dataset5['comment_count'])\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Year vs Like_Count\n",
    "train_dataset5=train_dataset4.sort_values('date',ascending=True)\n",
    "train_dataset5['like_count']=train_dataset5['like_count'].astype(float)\n",
    "train_dataset5['year']=pd.DatetimeIndex(train_dataset5['date']).year\n",
    "plt.plot(train_dataset5['year'],train_dataset5['like_count'])\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hour vs Like_Count\n",
    "train_dataset5=train_dataset4.sort_values('time',ascending=True)\n",
    "train_dataset5['like_count']=train_dataset5['like_count'].astype(float)\n",
    "train_dataset5['hour']=pd.DatetimeIndex(train_dataset5['time']).hour\n",
    "plt.plot(train_dataset5['hour'],train_dataset5['like_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hour vs forward_Count\n",
    "train_dataset5=train_dataset4.sort_values('time',ascending=True)\n",
    "train_dataset5['forward_count']=train_dataset5['forward_count'].astype(float)\n",
    "train_dataset5['hour']=pd.DatetimeIndex(train_dataset5['time']).hour\n",
    "plt.plot(train_dataset5['hour'],train_dataset5['forward_count'])\n",
    "plt.xticks(np.arange(0,23,1),rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hour vs comment_Count\n",
    "train_dataset5=train_dataset4.sort_values('time',ascending=True)\n",
    "train_dataset5['comment_count']=train_dataset5['comment_count'].astype(float)\n",
    "train_dataset5['hour']=pd.DatetimeIndex(train_dataset5['time']).hour\n",
    "plt.plot(train_dataset5['hour'],train_dataset5['comment_count'])\n",
    "plt.xticks(np.arange(0,23,1),rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Dataset Analysis\n",
    "Contribution: All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%pylab inline\n",
    "import copy\n",
    "from googletrans import Translator\n",
    "import emoji\n",
    "\n",
    "Populating the interactive namespace from numpy and matplotlib\n",
    "df1=pd.read_csv(\"G://DMA_PROJECT//weibo_train1.csv\")\n",
    "df2=pd.read_csv(\"G://DMA_PROJECT//weibo_train2.csv\")\n",
    "\n",
    "predict_dataset=pd.read_csv(\"G://DMA_PROJECT//weibo_predict.csv\")\n",
    "\n",
    "frames=[df1,df2]\n",
    "train_dataset=pd.concat(frames)\n",
    "train_dataset.head(30)\n",
    "predict_dataset.head(5)\n",
    "\n",
    "#translateion\n",
    "translator = Translator()\n",
    "predict_dataset[\"en-content\"] = predict_dataset['content'].head(10).map(lambda x: translator.translate(x, src=\"zh-CN\", dest=\"en\").text)\n",
    "predict_dataset.head(10)\n",
    "\n",
    "translator = Translator()\n",
    "train_dataset[\"en-content\"] = train_dataset['content'].head(10).map(lambda x: translator.translate(x, src=\"zh-CN\", dest=\"en\").text)\n",
    "train_dataset.head(10)\n",
    "\n",
    "\n",
    "train_dataset.head(614809).to_csv(\"G://DMA_PROJECT//weibo_train1_trans.csv\",sep=',',index=False,encoding='utf-8')\n",
    "train_dataset.tail(614809).to_csv(\"G://DMA_PROJECT//weibo_train2_trans.csv\",sep=',',index=False,encoding='utf-8')\n",
    "predict_dataset.to_csv(\"G://weibo_predict_trans.csv\",sep=',',index=False,encoding='utf-8')\n",
    "\n",
    "\n",
    "train_dataset=train_dataset.drop(['uni-content'],axis=1)\n",
    "tcount=train_dataset.groupby(by='u_id',as_index=False).agg({'content':pd.Series.nunique})\n",
    "print(tcount)\n",
    "\n",
    "tcount.to_csv(\"G://DMA_PROJECT//weibo_count_uid.csv\",index=False,encoding='utf-8')\n",
    "query= \"ffdd80d2f1023779e30956c34b044b25\"\n",
    "train_dataset[train_dataset['u_id']==query]\n",
    "pcount=predict_dataset.groupby(by='u_id',as_index=False).agg({'content':pd.Series.nunique})\n",
    "print(pcount)\n",
    "\n",
    "query= \"ffdd80d2f1023779e30956c34b044b25\"\n",
    "predict_dataset[predict_dataset['u_id']==query]\n",
    "tdcount=train_dataset.groupby(by='date',as_index=False).agg({'content':pd.Series.nunique})\n",
    "print(tdcount)\n",
    "\n",
    "\n",
    "tdcount.to_csv(\"G://DMA_PROJECT//weibo_train_date_content.csv\",index=False,encoding='utf-8')\n",
    "ttcount=train_dataset.groupby(by='time',as_index=False).agg({'content':pd.Series.nunique}\n",
    "print(ttcount)\n",
    "\n",
    "ttcount.to_csv(\"G://DMA_PROJECT//weibo_train_time_content.csv\",index=False,encoding='utf-8')\n",
    "pcount.to_csv(\"G://DMA_PROJECT//weibo_pcount_uid.csv\",index=False,encoding='utf-8')\n",
    "tlcount=train_dataset.groupby(by='u_id',as_index=False).agg({'like_count':pd.Series.nunique})\n",
    "print(tlcount)\n",
    "\n",
    "tccount=train_dataset.groupby(by='u_id',as_index=False).agg({'comment_count':pd.Series.nunique})\n",
    "print(tccount)\n",
    "\n",
    "\n",
    "tfcount=train_dataset.groupby(by='u_id',as_index=False).agg({'forward_count':pd.Series.nunique})\n",
    "print(tfcount)\n",
    "\n",
    "\n",
    "resultcount=pd.merge(tfcount,tccount,on='u_id')\n",
    "resultcount2=pd.merge(resultcount,tlcount,on='u_id')\n",
    "print(resultcount2)\n",
    "\n",
    "resultcount2.to_csv(\"G://DMA_PROJECT//weibo_tcount_fcl.csv\",index=False,encoding='utf-8')\n",
    "\n",
    "train_dataset['content_media_count']=train_dataset['content'].str.count('http')\n",
    "predict_dataset['content_media_count']=predict_dataset['content'].str.count('http')\n",
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------- DATA PRE-PROCESSING----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Best Statistical Factors\n",
    "Contribution: Ashish Kar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information on Loaded Modules\n",
    "We will find Mean, Median, Max and Min of Forward, Comment and Likes for every unique UID in train dataset for our further statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train_uid_stat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example For UID stats\n",
    "#### Say in train dataset, For UID x there are two MID(ie two posts):\n",
    "###### Train Dataset: \n",
    "![](u1.png)\n",
    "###### UID Stats:\n",
    "![](u2.png)\n",
    "#### Now Consider that same user has 4 mids in predict dataset, so prediction of FCL by factor \"mean\" will be as follows:\n",
    "###### Predict Dataset\n",
    "![](u3.png)\n",
    "#### Similary by factor \"max\" :\n",
    "###### Predict Dataset\n",
    "![](u4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with fixed Value\n",
    "##  1. Default Values\n",
    "About 80% of the training data are: 0 0 0 (forward_count,comment_count,like_count) and also, 96% of uid in predict dataset is    present in train dataset, for remaining 4% which are new, we need some default values.\n",
    "inspired by this, we try some fixed value for all uid:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to take Fixed FCL Values, Give Accuracy and Generate Predicted FCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@runTime\n",
    "def predict_with_fixed_value(forward,comment,like,submission=True):\n",
    "\t# type check\n",
    "\tif isinstance(forward,int) and isinstance(forward,int) and isinstance(forward,int):\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\traise TypeError(\"forward,comment,like should be type 'int' \")\n",
    "\t\n",
    "\ttraindata,testdata = loadData()\n",
    "\t\n",
    "\t#score on the training set\n",
    "\ttrain_real_pred = traindata[['forward_count','comment_count','like_count']]\n",
    "\ttrain_real_pred['fp'],train_real_pred['cp'],train_real_pred['lp'] = forward,comment,like\n",
    "\tprint (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))\n",
    "\t\n",
    "\t#predict on the test data with fixed value, generate submission file\n",
    "\tif submission:\n",
    "\t\ttest_pred = testdata[['u_id','m_id']]\n",
    "\t\ttest_pred['fp'],test_pred['cp'],test_pred['lp'] = forward,comment,like\n",
    "\t\t\n",
    "\t\tresult = []\n",
    "\t\tfilename = \"weibo_predict_{}_{}_{}.txt\".format(forward,comment,like)\n",
    "\t\tfor _,row in test_pred.iterrows():\n",
    "\t\t\tresult.append(\"{0}\\t{1}\\t{2},{3},{4}\\n\".format(row[0],row[1],row[2],row[3],row[4]))\n",
    "\t\tf = open(filename,'w')\n",
    "\t\tf.writelines(result)\n",
    "\t\tf.close()\n",
    "\t\tprint ('generate submission file \"{}\"'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UID Statistics (Mean, Max, Min, Median)\n",
    "Another wise solution is to predict respectively with uid's statistics(E.g mean,median)\t,\n",
    "their score on the training data:\n",
    "### Function to take Statistical Factor, Give Accuracy and Generate Predicted FCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@runTime\t\n",
    "def predict_with_stat(stat=\"median\",submission=True):\n",
    "\t\"\"\"\n",
    "\tstat:\n",
    "\t\tstring\n",
    "\t\tmin,max,mean,median\n",
    "\t\"\"\"\n",
    "\tstat_dic = genUidStat()\n",
    "\ttraindata,testdata = loadData()\n",
    "\t\n",
    "\t#get stat for each uid\n",
    "\tforward,comment,like = [],[],[]\n",
    "\tfor uid in traindata['u_id']:\n",
    "\t\tif uid in stat_dic:\n",
    "\t\t\tforward.append(int(stat_dic[uid][\"forward_\"+stat]))\n",
    "\t\t\tcomment.append(int(stat_dic[uid][\"comment_\"+stat]))\n",
    "\t\t\tlike.append(int(stat_dic[uid][\"like_\"+stat]))\n",
    "\t\telse:\n",
    "\t\t\tforward.append(0)\n",
    "\t\t\tcomment.append(0)\n",
    "\t\t\tlike.append(0)\n",
    "\t#score on the training set\n",
    "\ttrain_real_pred = traindata[['forward_count','comment_count','like_count']]\n",
    "\ttrain_real_pred['fp'],train_real_pred['cp'],train_real_pred['lp'] = forward,comment,like\n",
    "\tprint (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))\n",
    "\t\n",
    "\t#predict on the test data with fixed value, generate submission file\n",
    "\tif submission:\n",
    "\t\ttest_pred = testdata[['u_id','m_id']]\n",
    "\t\tforward,comment,like = [],[],[]\n",
    "\t\tfor uid in testdata['u_id']:\n",
    "\t\t\tif uid in stat_dic:\n",
    "\t\t\t\tforward.append(int(stat_dic[uid][\"forward_\"+stat]))\n",
    "\t\t\t\tcomment.append(int(stat_dic[uid][\"comment_\"+stat]))\n",
    "\t\t\t\tlike.append(int(stat_dic[uid][\"like_\"+stat]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tforward.append(0)\n",
    "\t\t\t\tcomment.append(0)\n",
    "\t\t\t\tlike.append(0)\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\ttest_pred['fp'],test_pred['cp'],test_pred['lp'] = forward,comment,like\n",
    "\t\t\n",
    "\t\tresult = []\n",
    "\t\tfilename = \"weibo_predict_{}.txt\".format(stat)\n",
    "\t\tfor _,row in test_pred.iterrows():\n",
    "\t\t\tresult.append(\"{0}\\t{1}\\t{2},{3},{4}\\n\".format(row[0],row[1],row[2],row[3],row[4]))\n",
    "\t\tf = open(filename,'w')\n",
    "\t\tf.writelines(result)\n",
    "\t\tf.close()\n",
    "\t\tprint ('generate submission file \"{}\"'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_stat(stat=\"median\",submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_fixed_value(0,1,1,submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_stat(stat=\"mean\",submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_stat(stat=\"max\",submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_stat(stat=\"min\",submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_fixed_value(0,0,0,submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_fixed_value(0,0,1,submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_fixed_value(0,1,0,submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_fixed_value(1,0,0,submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_fixed_value(1,0,1,submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_fixed_value(1,1,0,submission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t\tpredict_with_fixed_value(1,1,1,submission=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "![](overall1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate More Factors from Content ( Media and Symbol Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%pylab inline\n",
    "import copy\n",
    "from googletrans import Translator\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "df1=pd.read_csv(\"G://DMA_PROJECT//weibo_train1.csv\")\n",
    "df2=pd.read_csv(\"G://DMA_PROJECT//weibo_train2.csv\")\n",
    "frames=[df1,df2]\n",
    "train_dataset=pd.concat(frames)\n",
    "predict_dataset=pd.read_csv(\"G://DMA_PROJECT//weibo_predict.csv\")\n",
    "\n",
    "train_dataset.head(10)\n",
    "\n",
    "train_dataset['content_media_count'].value_counts()\n",
    "\n",
    "train_dataset['content_media_count'].value_counts().plot(kind='bar')\n",
    "\n",
    "train_dataset=train_dataset.sort_values('content_media_count',ascending=True)\n",
    "train_dataset['like_count']=train_dataset['like_count'].astype(float)\n",
    "plt.plot(train_dataset['content_media_count'],train_dataset['like_count'])\n",
    "plt.xticks(np.arange(0,10,1))\n",
    "\n",
    "train_dataset=train_dataset.sort_values('content_media_count',ascending=True)\n",
    "train_dataset['forward_count']=train_dataset['forward_count'].astype(float)\n",
    "plt.plot(train_dataset['content_media_count'],train_dataset['forward_count'])\n",
    "plt.xticks(np.arange(0,10,1))\n",
    "\n",
    "train_dataset=train_dataset.sort_values('content_media_count',ascending=True)\n",
    "train_dataset['comment_count']=train_dataset['comment_count'].astype(float)\n",
    "plt.plot(train_dataset['content_media_count'],train_dataset['comment_count'])\n",
    "plt.xticks(np.arange(0,10,1))\n",
    "\n",
    "train_dataset['content_#_count'].value_counts().plot(kind='bar')\n",
    "\n",
    "train_dataset['content_#_count']=train_dataset['non_emoji_content'].str.count(\"#\")\n",
    "train_dataset['content_@_count']=train_dataset['non_emoji_content'].str.count(\"@\")\n",
    "train_dataset['content_?_count']=train_dataset['non_emoji_content'].str.count(\"\\?\")\n",
    "train_dataset['content_!_count']=train_dataset['non_emoji_content'].str.count(\"!\")\n",
    "\n",
    "train_dataset.head(10)\n",
    "\n",
    "train_dataset['content_length']=train_dataset['content'].str.len()\n",
    "\n",
    "train_dataset.head(10)\n",
    "\n",
    "\n",
    "train_dataset['emoji_count']=train_dataset['non_emoji_content'].str.count(str(emoji.UNICODE_EMOJI.keys()))\n",
    "\n",
    "train_dataset.head(30)\n",
    "\n",
    "print(emoji.UNICODE_EMOJI.keys())\n",
    "\n",
    "import re\n",
    "train_dataset['emoji']=train_dataset['content_spchar'].str.replace(r'[\\U0001F602-\\U0001F64F]','emoticon')\n",
    "\n",
    "train_dataset.head(30)\n",
    "\n",
    "train_dataset['emoji_count']=train_dataset['emoji'].str.count(\"emoticon\")\n",
    "\n",
    "train_dataset.head(30)\n",
    "\n",
    "train_dataset.drop('content_spchar',axis=1,inplace=True)\n",
    "train_dataset.drop('non_emoji_content',axis=1,inplace=True)\n",
    "train_dataset.drop('emoji',axis=1,inplace=True)\n",
    "train_dataset.rename(columns={'emoji_count':'content_emoji_count'},inplace=True)\n",
    "\n",
    "train_dataset.head(30)\n",
    "\n",
    "train_dataset.head(614809).to_csv(\"G://DMA_PROJECT//weibo_train1_cp.csv\",sep=',',index=False,encoding='utf-8')\n",
    "train_dataset.tail(614809).to_csv(\"G://DMA_PROJECT//weibo_train2_cp.csv\",sep=',',index=False,encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "Contribution: Arundati Dixit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= pd.read_fwf(\"G:\\\\weibo_train_data.txt\",header=None,names=Names ,encoding='utf-8',delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern=re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         \"]+\",flags=re.UNICODE)\n",
    "train_dataset2['content']=emoji_pattern.sub(r'',str(train_dataset2['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_dataframe = pd.DataFrame(data=train_dataset2['content'].head(30))\n",
    "translator = Translator()\n",
    "translate_dataframe[\"English_content\"] = translate_dataframe['content'].map(lambda x: translator.translate(x, src=\"zh-CN\", dest=\"en\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data is split into 400 chunks and translated individually and concatenated back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,400):\n",
    "    filename=\"G:\\\\concatfiles\\\\f\"+str(j)+\".txt\"\n",
    "    transname=\"G:\\\\translated\\\\ts\"+str(j)+\".zh-CN.en.txt\"\n",
    "    print(filename)\n",
    "    print(transname)\n",
    "    f=pd.read_csv(filename)\n",
    "    t= pd.read_csv(transname,sep=\"5A09\")\n",
    "    frames = [f,t]\n",
    "\n",
    "    df=(pd.concat(frames, join='outer', ignore_index=False,keys=None, levels=None, names=None, verify_integrity=False, copy=True, axis=1))\n",
    "    translated=translated.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "Contribution: Apoorva Malemath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%pylab inline\n",
    "import copy\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import jieba\n",
    "import time\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.externals import joblib\n",
    "from nltk.corpus import stopwords as e_stopwords\n",
    "from datetime import datetime, timedelta\n",
    "import jieba\n",
    "import sys\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1= pd.read_csv(\"G:\\\\preprocessed_1.csv\")\n",
    "train2= pd.read_csv(\"G:\\\\preprocessed_2.csv\")\n",
    "frames=[train1,train2]\n",
    "train=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT PREPROCESSING\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVAL OF NOISE - URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remurl(content):\n",
    "    try:\n",
    "        URLless_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', content)\n",
    "        return URLless_string\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urlrem = pd.DataFrame(columns=['en_contenturl','url_rem'])\n",
    "for i in range(100000):\n",
    "    non_emo=translated['en_content'].iloc[i]\n",
    "    content=translated['en_content'].iloc[i]\n",
    "    new_content=remurl(content)\n",
    "    \n",
    "    df_urlrem = df_urlrem.append({'en_contenturl': non_emo,'url_rem':new_content}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def rem_num(tokens):\n",
    "        try:\n",
    "            for item in tokens:\n",
    "                if item.isdigit():    \n",
    "                    tokens.remove(item)\n",
    "            return tokens\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remnum = pd.DataFrame(columns=['url_rem',])\n",
    "for i in range(100000):\n",
    "    content=df_urlrem['url_rem'].iloc[i]\n",
    "    nonum=rem_num(df_urlrem['url_rem'].iloc[i])\n",
    "    list1=[content,nonum]\n",
    "    df_remnum = df_remnum.append({'url_rem': content, 'no_num': nonum}, ignore_index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVAL OF STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remStopword=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(data):\n",
    "    stopw=\"STOPWORDS.txt\"\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    words = word_tokenize(data)\n",
    "    wordsFiltered = []\n",
    "    try:\n",
    "        for w in words:\n",
    "            if (w not in stopw or w not in stop_words) :\n",
    "                wordsFiltered.append(w)\n",
    "        return wordsFiltered\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(columns=['no_num','Stopwrod_removed'])\n",
    "for i in range(100000):\n",
    "    non_emo=df_remnum['no_num'].iloc[i]\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",\" \",str(df_remnum['no_num'].iloc[i]))\n",
    "    remStopword=removeStopwords(letters_only)\n",
    "    list1=[non_emo,remStopword]\n",
    "    df_new = df_new.append({'no_num': non_emo, 'Stopword_removed': remStopword}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    # First Word tokenization\n",
    "    nltk_tokens =tokens \n",
    "    stem = []\n",
    "    #Next find the roots of the word\n",
    "    try:\n",
    "        for w in nltk_tokens:\n",
    "            s=porter_stemmer.stem(w)\n",
    "            stem.append(s)\n",
    "        return stem\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem = pd.DataFrame(columns=['en_contentst','Stemming'])\n",
    "for i in range(100000):\n",
    "    content=df_new['Stopword_removed'].iloc[i]\n",
    "    stem=stemming(df_new['Stopword_removed'].iloc[i])\n",
    "    list1=[content,stem]\n",
    "    df_stem = df_stem.append({'en_contentst': content, 'Stemming': stem}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LEMMATIZATION\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(tokens):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nltk_tokens =tokens \n",
    "    lem = []\n",
    "    #Next find the roots of the word\n",
    "    try:\n",
    "        for w in nltk_tokens:\n",
    "            l=wordnet_lemmatizer.lemmatize(w)\n",
    "            lem.append(l)\n",
    "        return lem\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem = pd.DataFrame(columns=['Stemmingle','lemmatization'])\n",
    "for i in range(100000):\n",
    "    content=df_stem['Stemming'].iloc[i]\n",
    "    lem=stemming(df_stem['Stemming'].iloc[i])\n",
    "    list1=[content,lem]\n",
    "    df_lem = df_lem.append({'Stemmingle': content, 'lemmatization': lem}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolower(tokens):\n",
    "    try:\n",
    "        nltk_tokens=tokens\n",
    "        x = [element.lower() for element in nltk_tokens] \n",
    "        return x\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lower = pd.DataFrame(columns=['lemmatizationtl','lower'])\n",
    "for i in range(100000):\n",
    "    content=df_lem['lemmatization'].iloc[i]\n",
    "    low=tolower(df_lem['lemmatization'].iloc[i])\n",
    "    list1=[content,low]\n",
    "    df_lower = df_lower.append({'lemmatizationtl': content, 'lower': low}, ignore_index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE PUNTUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def rem_punctuation(tokens):\n",
    "        try:\n",
    "            input_text = ' '.join(tokens).lower()\n",
    "            s = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", input_text)\n",
    "            #print(input_text)\n",
    "            words = word_tokenize(s)\n",
    "            return words\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rempunc = pd.DataFrame(columns=['lemmatizationtlp','no_punc'])\n",
    "for i in range(100000):\n",
    "    content=df_lower['lemmatizationtl'].iloc[i]\n",
    "    nopun=rem_punctuation(df_lower['lemmatizationtl'].iloc[i])\n",
    "    list1=[content,nopun]\n",
    "    df_rempunc = df_rempunc.append({'lemmatizationtlp': content, 'no_punc': nopun}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[translated,df_urlrem, df_new, df_stem, df_lem, df_lower, df_remnum, df_rempunc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=(pd.concat(frames, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------MODEL BUILDING-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW\n",
    "Contribution: Apoorva Malemath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "  We convert text to a numerical representation called a feature vector. \n",
    "  A feature vector can be as simple as a list of numbers.\n",
    "  \n",
    "  The bag-of-words model is one of the feature extraction algorithms for text.\n",
    "\n",
    "1.The first step in this model is defining the vocabulary\n",
    "\n",
    "2.The second step is to convert sentences into a frequency vector based on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from document\n",
    "import pandas as pd\n",
    "df_pre=pd.read_csv(\"E:\\\\DMA_PRE\\\\PREPROCESSED.csv\")\n",
    "df=pd.read_csv(\"E:\\\\DMA_PRE\\\\PREPROCESSED.csv\")\n",
    "#Adjustments to be done for the data\n",
    "df['content']=df['content'].str.replace(\",\", \"\")\n",
    "df['content']=df['content'].str.replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list for all content\n",
    "l=[]\n",
    "for i in range(0,10000):\n",
    "    l.append(df['content'].iloc[i])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for bag of words model\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#for building vocabulary\n",
    "def tokenize_sentences(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = extract_words(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def extract_words(sentence):\n",
    "    ignore_words = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned    \n",
    "\n",
    "#function which returns feature vector\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence)\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words),dtype=int)\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the vocabulary for the list created\n",
    "vocabulary1 = tokenize_sentences(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [x for x in vocabulary1 if not (x.isdigit() or x[1:].isdigit())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing bag of words\n",
    "a=[]\n",
    "\n",
    "for i in range(0,10000):\n",
    "    #b.append(bagofwords(df['content'].iloc[i], vocabulary1),ignore_index=True)\n",
    "    a.append(bagofwords(df['content'].iloc[i], vocabulary1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=np.asarray(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol=pd.read_csv(\"E:\\\\DMA_PRE\\\\weibo_polarity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model with BOW appened with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1=np.insert(bow,16792,df_pol[\"content_media_count\"],axis=1)\n",
    "bow2=np.insert(bow1,16793,df_pol[\"forward_median\"],axis=1)\n",
    "bow3=np.insert(bow2,16794,df_pol[\"comment_median\"],axis=1)\n",
    "bow4=np.insert(bow3,16795,df_pol[\"like_median\"],axis=1)\n",
    "bow5=np.insert(bow4,16796,df_pol[\"polarity\"],axis=1)\n",
    "\n",
    "X_train1=train_bow\n",
    "X_test1=pred_bow\n",
    "Y_train1=train_df[[\"forward_count\",\"like_count\",\"comment_count\"]]\n",
    "Y_test1=predict_df[[\"forward_count\"]]\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train1,Y_train1)\n",
    "pred1=lm.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "\n",
    "np.savetxt(\"E://DMA_PRE//weibo_predict_resultbow.csv\",pred1,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result=pd.read_csv(\"E://DMA_PRE//weibo_predict_resultbow.csv\")\n",
    "\n",
    "train_real_pred = Y_test1\n",
    "train_real_pred['fp']=result['forward_count'].values\n",
    "train_real_pred['cp']=result['comment_count'].values\n",
    "train_real_pred['lp']=result['like_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS USING COUNTER VECTORIZER\n",
    "Contribution: Apoorva Malemath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('E:\\\\DMA_PRE\\\\pre_bow.csv')\n",
    "train_df=df1[0:8000]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l=[]\n",
    "for i in range(0,8000):\n",
    "    train_l.append(df_pre['content'].iloc[i])\n",
    "len(train_l)\n",
    "pred_l=[]\n",
    "for i in range(8001,10000):\n",
    "    pred_l.append(df_pre['content'].iloc[i])\n",
    "len(pred_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=CountVectorizer()\n",
    "vect.fit(train_l)\n",
    "x=vect.transform(train_l)\n",
    "vect = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_train_data_features = vect.fit_transform(train_l)\n",
    "off_train_data_features = off_train_data_features.toarray()\n",
    "off_train_data_forward = train_df.forward_count\n",
    "\n",
    "off_test_data_features = vect.fit_transform(pred_l)\n",
    "off_test_data_features = off_test_data_features.toarray()\n",
    "off_test_data_forward = predict_df.forward_count\n",
    "\n",
    "X_train1=off_train_data_features\n",
    "X_test1= off_test_data_features\n",
    "Y_train1=dftrain[[\"forward_count\",\"like_count\",\"comment_count\"]]\n",
    "Y_test1=dfcv[[\"forward_count\",\"like_count\",\"comment_count\"]]\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train1,Y_train1)\n",
    "pred1=lm.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "np.savetxt(\"E://DMA_PRE//weibo_predict_resultbow1.csv\",pred1,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result1=pd.read_csv(\"E://DMA_PRE//weibo_predict_resultbow1.csv\")\n",
    "result1=result1.abs()\n",
    "result1=result1.astype(int)\n",
    "train_real_pred = Y_test1\n",
    "train_real_pred['fp']=result1['forward_count'].values\n",
    "train_real_pred['cp']=result1['comment_count'].values\n",
    "train_real_pred['lp']=result1['like_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_pol[0:8000]\n",
    "cv=df_pol[8001:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_train_data_features = vect.fit_transform(train_l)\n",
    "off_train_data_features = off_train_data_features.toarray()\n",
    "\n",
    "off_train_data_features1=np.insert(off_train_data_features,100,train[\"content_media_count\"],axis=1)\n",
    "off_train_data_features2=np.insert(off_train_data_features1,101,train[\"forward_median\"],axis=1)\n",
    "off_train_data_features3=np.insert(off_train_data_features2,102,train[\"comment_median\"],axis=1)\n",
    "off_train_data_features4=np.insert(off_train_data_features3,103,train[\"like_median\"],axis=1)\n",
    "#off_train_data_features5=np.insert(off_train_data_features4,100,train[\"polarity\"],axis=1)\n",
    "off_train_data_features6=np.insert(off_train_data_features4,104,train[\"content_emoji_count\"],axis=1)\n",
    "#off_train_data_forward = train_df.forward_count\n",
    "\n",
    "off_test_data_features = vect.fit_transform(pred_l)\n",
    "off_test_data_features = off_test_data_features.toarray()\n",
    "off_test_data_features1=np.insert(off_test_data_features,100,cv[\"content_media_count\"],axis=1)\n",
    "off_test_data_features2=np.insert(off_test_data_features1,101,cv[\"forward_median\"],axis=1)\n",
    "off_test_data_features3=np.insert(off_test_data_features2,102,cv[\"comment_median\"],axis=1)\n",
    "off_test_data_features4=np.insert(off_test_data_features3,103,cv[\"like_median\"],axis=1)\n",
    "#off_test_data_features5=np.insert(off_test_data_features4,100,cv[\"polarity\"],axis=1)\n",
    "off_test_data_features6=np.insert(off_test_data_features4,104,cv[\"content_emoji_count\"],axis=1)\n",
    "#off_test_data_forward = predict_df.forward_count\n",
    "\n",
    "X_train1=off_train_data_features6\n",
    "X_test1= off_test_data_features6\n",
    "Y_train1=dftrain[[\"forward_count\",\"like_count\",\"comment_count\"]]\n",
    "Y_test1=dfcv[[\"forward_count\",\"like_count\",\"comment_count\"]]\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train1,Y_train1)\n",
    "pred1=lm.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "\n",
    "np.savetxt(\"E://DMA_PRE//weibo_predict_resultbow3.csv\",pred1,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result3=pd.read_csv(\"E://DMA_PRE//weibo_predict_resultbow3.csv\")\n",
    "result3=result3.abs()\n",
    "result3=result3.astype(int)\n",
    "train_real_pred = Y_test1\n",
    "train_real_pred['fp']=result3['forward_count'].values\n",
    "train_real_pred['cp']=result3['comment_count'].values\n",
    "train_real_pred['lp']=result3['like_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_train_data_features = vect.fit_transform(train_l)\n",
    "off_train_data_features = off_train_data_features.toarray()\n",
    "\n",
    "off_train_data_features1=np.insert(off_train_data_features,100,train[\"content_media_count\"],axis=1)\n",
    "off_train_data_features2=np.insert(off_train_data_features1,101,train[\"forward_median\"],axis=1)\n",
    "off_train_data_features3=np.insert(off_train_data_features2,102,train[\"comment_median\"],axis=1)\n",
    "off_train_data_features4=np.insert(off_train_data_features3,103,train[\"like_median\"],axis=1)\n",
    "off_train_data_features5=np.insert(off_train_data_features4,104,train[\"polarity\"],axis=1)\n",
    "\n",
    "#off_train_data_forward = train_df.forward_count\n",
    "\n",
    "off_test_data_features = vect.fit_transform(pred_l)\n",
    "off_test_data_features = off_test_data_features.toarray()\n",
    "off_test_data_features1=np.insert(off_test_data_features,100,cv[\"content_media_count\"],axis=1)\n",
    "off_test_data_features2=np.insert(off_test_data_features1,101,cv[\"forward_median\"],axis=1)\n",
    "off_test_data_features3=np.insert(off_test_data_features2,102,cv[\"comment_median\"],axis=1)\n",
    "off_test_data_features4=np.insert(off_test_data_features3,103,cv[\"like_median\"],axis=1)\n",
    "off_test_data_features5=np.insert(off_test_data_features4,104,cv[\"polarity\"],axis=1)\n",
    "\n",
    "#off_test_data_forward = predict_df.forward_count\n",
    "\n",
    "X_train1=off_train_data_features4\n",
    "X_test1= off_test_data_features4\n",
    "Y_train1=dftrain[\"forward_count\"]\n",
    "Y_test1=dfcv[\"forward_count\"]\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train1,Y_train1)\n",
    "pred1=lm.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "\n",
    "np.savetxt(\"E://DMA_PRE//weibo_predict_resultbow4.csv\",pred1,delimiter=',',header=\"forward_count\",comments=\"\")\n",
    "result4=pd.read_csv(\"E://DMA_PRE//weibo_predict_resultbow4.csv\")\n",
    "result4=result4.abs()\n",
    "result4=result4.astype(int)\n",
    "\n",
    "train_real_pred = Y_test1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_train_data_features = vect.fit_transform(train_l)\n",
    "off_train_data_features = off_train_data_features.toarray()\n",
    "\n",
    "off_train_data_features1=np.insert(off_train_data_features,100,train[\"content_media_count\"],axis=1)\n",
    "off_train_data_features2=np.insert(off_train_data_features1,101,train[\"forward_median\"],axis=1)\n",
    "off_train_data_features3=np.insert(off_train_data_features2,102,train[\"comment_median\"],axis=1)\n",
    "off_train_data_features4=np.insert(off_train_data_features3,103,train[\"like_median\"],axis=1)\n",
    "off_train_data_features5=np.insert(off_train_data_features4,100,train[\"polarity\"],axis=1)\n",
    "#off_train_data_forward = train_df.forward_count\n",
    "\n",
    "off_test_data_features = vect.fit_transform(pred_l)\n",
    "off_test_data_features = off_test_data_features.toarray()\n",
    "off_test_data_features1=np.insert(off_test_data_features,100,cv[\"content_media_count\"],axis=1)\n",
    "off_test_data_features2=np.insert(off_test_data_features1,101,cv[\"forward_median\"],axis=1)\n",
    "off_test_data_features3=np.insert(off_test_data_features2,102,cv[\"comment_median\"],axis=1)\n",
    "off_test_data_features4=np.insert(off_test_data_features3,103,cv[\"like_median\"],axis=1)\n",
    "off_test_data_features5=np.insert(off_test_data_features4,100,cv[\"polarity\"],axis=1)\n",
    "#off_test_data_forward = predict_df.forward_count\n",
    "\n",
    "X_train2=off_train_data_features4\n",
    "X_test2= off_test_data_features4\n",
    "Y_train2=dftrain[\"like_count\"]\n",
    "Y_test2=dfcv[\"like_count\"]\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train1,Y_train1)\n",
    "pred1=lm.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "\n",
    "np.savetxt(\"E://DMA_PRE//weibo_predict_resultbow5.csv\",pred1,delimiter=',',header=\"comment_count\",comments=\"\")\n",
    "result5=pd.read_csv(\"E://DMA_PRE//weibo_predict_resultbow5.csv\")\n",
    "result5=result5.abs()\n",
    "result5=result5.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_train_data_features = vect.fit_transform(train_l)\n",
    "off_train_data_features = off_train_data_features.toarray()\n",
    "\n",
    "off_train_data_features1=np.insert(off_train_data_features,100,train[\"content_media_count\"],axis=1)\n",
    "off_train_data_features2=np.insert(off_train_data_features1,101,train[\"forward_median\"],axis=1)\n",
    "off_train_data_features3=np.insert(off_train_data_features2,102,train[\"comment_median\"],axis=1)\n",
    "off_train_data_features4=np.insert(off_train_data_features3,103,train[\"like_median\"],axis=1)\n",
    "off_train_data_features5=np.insert(off_train_data_features4,100,train[\"polarity\"],axis=1)\n",
    "#off_train_data_forward = train_df.forward_count\n",
    "\n",
    "off_test_data_features = vect.fit_transform(pred_l)\n",
    "off_test_data_features = off_test_data_features.toarray()\n",
    "off_test_data_features1=np.insert(off_test_data_features,100,cv[\"content_media_count\"],axis=1)\n",
    "off_test_data_features2=np.insert(off_test_data_features1,101,cv[\"forward_median\"],axis=1)\n",
    "off_test_data_features3=np.insert(off_test_data_features2,102,cv[\"comment_median\"],axis=1)\n",
    "off_test_data_features4=np.insert(off_test_data_features3,103,cv[\"like_median\"],axis=1)\n",
    "off_test_data_features5=np.insert(off_test_data_features4,100,cv[\"polarity\"],axis=1)\n",
    "#off_test_data_forward = predict_df.forward_count\n",
    "\n",
    "X_train3=off_train_data_features4\n",
    "X_test3= off_test_data_features4\n",
    "Y_train3=dftrain[\"comment_count\"]\n",
    "Y_test3=dfcv[\"comment_count\"]\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train1,Y_train1)\n",
    "pred1=lm.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "\n",
    "np.savetxt(\"E://DMA_PRE//weibo_predict_resultbow6.csv\",pred1,delimiter=',',header=\"like_count\",comments=\"\")\n",
    "result6=pd.read_csv(\"E://DMA_PRE//weibo_predict_resultbow6.csv\")\n",
    "result6=result6.abs()\n",
    "result6=result6.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_real_pred = pd.concat([Y_test1,Y_test2,Y_test3],axis=1)\n",
    "train_real_pred['fp']=result4['forward_count'].values\n",
    "train_real_pred['cp']=result5['comment_count'].values\n",
    "train_real_pred['lp']=result6['like_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity\n",
    "Contribution: Deepti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import import_ipynb\n",
    "from evaluation import precision\n",
    "from runTime import runTime\n",
    "\n",
    "df=pd.read_csv(\"G://preprocessed1L.csv\")\n",
    "df_new = pd.DataFrame(columns=['pol'])\n",
    "\n",
    "for i in range(0,100000):\n",
    "    try:\n",
    "        a=TextBlob(df['no_punc'].iloc[i]).sentiment\n",
    "        df_new=df_new.append({'pol':a[0]}, ignore_index=True) \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        df_new=df_new.append({'pol':999999}, ignore_index=True) \n",
    "\n",
    "df['polarity']=df_new['pol']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Modelling\n",
    "Contribution: All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train1_cp.csv\")\n",
    "df2=pd.read_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train2_cp.csv\")\n",
    "frames=[df1,df2]\n",
    "train_dataset=pd.concat(frames)\n",
    "predict_dataset=pd.read_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_predict_cp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['date']=pd.to_datetime(train_dataset['date'],errors='coerce')\n",
    "train_month=[g for n, g in train_dataset.groupby(pd.Grouper(key='date',freq='M'))]\n",
    "train_dataset['time']=pd.to_datetime(train_dataset['time'],errors='coerce')\n",
    "train_hour=[g for n, g in train_dataset.groupby(pd.Grouper(key='time',freq='H'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_month[0].to_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train_feb_cp.csv\",sep=',',index=False,encoding='utf-8')\n",
    "train_month[1].to_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train_march_cp.csv\",sep=',',index=False,encoding='utf-8')\n",
    "train_month[2].to_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train_april_cp.csv\",sep=',',index=False,encoding='utf-8')\n",
    "train_month[3].to_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train_may_cp.csv\",sep=',',index=False,encoding='utf-8')\n",
    "train_month[4].to_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train_june_cp.csv\",sep=',',index=False,encoding='utf-8')\n",
    "train_month[5].to_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train_july_cp.csv\",sep=',',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for i in range(0,24):\n",
    "    path=\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_train_hour_\"+str(i)+\"_cp.csv\"\n",
    "    train_hour[i].to_csv(path,sep=',',index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames1=[train_month[0],train_month[1],train_month[2],train_month[3],train_month[4]]\n",
    "train=pd.concat(frames1)\n",
    "predict=train_month[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial Predictions without Model and Analysis\n",
    "### Putting known values of stats in predict dataset without any computation and finding accuracy\n",
    "### Best Statistical Factors and Default Value\n",
    "![](overall1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 (Factors: Media, #, @, ?, !, Length, Emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"content_media_count\",\"content_#_count\",\"content_@_count\",\"content_?_count\",\"content_!_count\",\"content_length\",\"content_emoji_count\"]]\n",
    "Y_train=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test=predict[[\"content_media_count\",\"content_#_count\",\"content_@_count\",\"content_?_count\",\"content_!_count\",\"content_length\",\"content_emoji_count\"]]\n",
    "Y_test=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred=lm.predict(X_test)\n",
    "pred=pred.round()\n",
    "pred=(np.maximum(pred,0.))\n",
    "np.savetxt(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_predict_result2.csv\",pred,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result=pd.read_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_predict_result2.csv\")\n",
    "train_real_pred = Y_test\n",
    "forward=result['forward_count'].values\n",
    "comment=result['forward_count'].values\n",
    "like=result['forward_count'].values\n",
    "train_real_pred['fp'],train_real_pred['cp'],train_real_pred['lp'] = forward,comment,like\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 (Media, Length, Emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"content_media_count\",\"content_length\",\"content_emoji_count\"]]\n",
    "Y_train=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test=predict[[\"content_media_count\",\"content_length\",\"content_emoji_count\"]]\n",
    "Y_test=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred=lm.predict(X_test)\n",
    "pred=pred.round()\n",
    "pred=(np.maximum(pred,0.))\n",
    "np.savetxt(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_predict_result3.csv\",pred,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result=pd.read_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_predict_result3.csv\")\n",
    "train_real_pred = Y_test\n",
    "forward=result['forward_count'].values\n",
    "comment=result['forward_count'].values\n",
    "like=result['forward_count'].values\n",
    "train_real_pred['fp'],train_real_pred['cp'],train_real_pred['lp'] = forward,comment,like\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3(Media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"content_media_count\"]]\n",
    "Y_train=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test=predict[[\"content_media_count\"]]\n",
    "Y_test=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred=lm.predict(X_test)\n",
    "pred=pred.round()\n",
    "pred=(np.maximum(pred,0.))\n",
    "np.savetxt(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_predict_result4.csv\",pred,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result=pd.read_csv(\"E:\\\\5th Sem\\\\DMA Project\\\\Model Evaluation\\\\weibo_predict_result4.csv\")\n",
    "train_real_pred = Y_test\n",
    "train_real_pred['fp']=result['forward_count'].values\n",
    "train_real_pred['cp']=result['comment_count'].values\n",
    "train_real_pred['lp']=result['like_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 (Time) Pre-requisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"hour\",\"min\",\"sec\"]]\n",
    "Y_train=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test=predict[[\"hour\",\"min\",\"sec\"]]\n",
    "Y_test=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred=lm.predict(X_test)\n",
    "pred=pred.round()\n",
    "pred=(np.maximum(pred,0.))\n",
    "np.savetxt(\"G://DMA_PROJECT//weibo_predict_result5.csv\",pred,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result=pd.read_csv(\"G://DMA_PROJECT//weibo_predict_result5.csv\")\n",
    "train_real_pred = Y_test\n",
    "train_real_pred['fp']=result['forward_count'].values\n",
    "train_real_pred['cp']=result['comment_count'].values\n",
    "train_real_pred['lp']=result['like_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5 (Time: Hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"hour\"]]\n",
    "Y_train=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test=predict[[\"hour\"]]\n",
    "Y_test=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred=lm.predict(X_test)\n",
    "pred=pred.round()\n",
    "pred=(np.maximum(pred,0.))\n",
    "np.savetxt(\"G://DMA_PROJECT//weibo_predict_result6.csv\",pred,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result=pd.read_csv(\"G://DMA_PROJECT//weibo_predict_result6.csv\")\n",
    "train_real_pred = Y_test\n",
    "train_real_pred['fp']=result['forward_count'].values\n",
    "train_real_pred['cp']=result['comment_count'].values\n",
    "train_real_pred['lp']=result['like_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6  Time: (Hour, Min, Sec), Media, Length, Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"content_media_count\",\"content_length\",\"content_emoji_count\",\"hour\",\"min\",\"sec\"]]\n",
    "Y_train=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test=predict[[\"content_media_count\",\"content_length\",\"content_emoji_count\",\"hour\",\"min\",\"sec\"]]\n",
    "Y_test=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred=lm.predict(X_test)\n",
    "pred=pred.round()\n",
    "pred=(np.maximum(pred,0.))\n",
    "np.savetxt(\"G://DMA_PROJECT//weibo_predict_result7.csv\",pred,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result=pd.read_csv(\"G://DMA_PROJECT//weibo_predict_result7.csv\")\n",
    "train_real_pred = Y_test\n",
    "train_real_pred['fp']=result['forward_count'].values\n",
    "train_real_pred['cp']=result['comment_count'].values\n",
    "train_real_pred['lp']=result['like_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7 Median,Time: (Hour, Min, Sec), Media, Length, Emoji \n",
    "## Only for Forward Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"content_media_count\",\"content_length\",\"content_emoji_count\",\"hour\",\"min\",\"sec\",\"forward_median\"]]\n",
    "Y_train=train[[\"forward_count\"]]\n",
    "X_test=predict[[\"content_media_count\",\"content_length\",\"content_emoji_count\",\"hour\",\"min\",\"sec\",\"forward_median\"]]\n",
    "Y_test=predict[[\"forward_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred=lm.predict(X_test)\n",
    "pred=pred.round()\n",
    "pred=(np.maximum(pred,0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"G://DMA_PROJECT//weibo_predict_result8.csv\",pred,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result=pd.read_csv(\"G://DMA_PROJECT//weibo_predict_result8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_real_pred=Y_test\n",
    "train_real_pred['fp']=result['forward_count'].values\n",
    "train_real_pred=train_real_pred.round()\n",
    "print (\"Score on the training set:{0:.2f}%\".format(precision2(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling with Polarity \n",
    "-10000 tuples\n",
    "\n",
    "Contribution: Deepti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpol=pd.read_csv(\"E:\\DMA_PRE\\polarity\\weibo_polarity.csv\")\n",
    "dfpol['date']=pd.to_datetime(dfpol['date'],errors='coerce')\n",
    "train_month=[g for n, g in dfpol.groupby(pd.Grouper(key='date',freq='M'))]\n",
    "\n",
    "train_month[0]=pd.read_csv(\"E:\\DMA_PRE\\polarity\\weibo_train_feb_cpts10000.csv\")\n",
    "train_month[1]=pd.read_csv(\"E:\\DMA_PRE\\polarity\\weibo_train_march_cpts10000.csv\")\n",
    "train_month[2]=pd.read_csv(\"E:\\DMA_PRE\\polarity\\weibo_train_april_cpts10000.csv\")\n",
    "train_month[3]=pd.read_csv(\"E:\\DMA_PRE\\polarity\\weibo_train_may_cpts10000.csv\")\n",
    "train_month[4]=pd.read_csv(\"E:\\DMA_PRE\\polarity\\weibo_train_june_cpts10000.csv\")\n",
    "train_month[5]=pd.read_csv(\"E:\\DMA_PRE\\polarity\\weibo_train_july_cpts10000.csv\")\n",
    "\n",
    "\n",
    "frames1=[train_month[0],train_month[1],train_month[2],train_month[3],train_month[4]]\n",
    "train=pd.concat(frames1)\n",
    "predict=train_month[5]\n",
    "\n",
    "## Model 7: (Factors: Media, Length, Emoji, Median,Polarity)\n",
    "\n",
    "X_train1=train[[\"content_media_count\",\"content_length\",\"forward_median\",\"comment_median\",\"like_median\",\"polarity\"]]\n",
    "Y_train1=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test1=predict[[\"content_media_count\",\"content_length\",\"forward_median\",\"comment_median\",\"like_median\",\"polarity\"]]\n",
    "Y_test1=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train1.fillna(X_train1.max(),inplace=True)\n",
    "X_test1.fillna(X_test1.max(),inplace=True)\n",
    "\n",
    "lm1=linear_model.LinearRegression()\n",
    "model1=lm1.fit(X_train1,Y_train1)\n",
    "pred1=lm1.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "\n",
    "print(model1.coef_)\n",
    "print(model1.intercept_)\n",
    "\n",
    "np.savetxt(\"E:\\DMA_PRE\\polarity\\weibo_predict_result51.csv\",pred1,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result1=pd.read_csv(\"E:\\DMA_PRE\\polarity\\weibo_predict_result51.csv\")\n",
    "\n",
    "print(mean_squared_error(Y_test1,result1))\n",
    "\n",
    "train_real_pred=Y_test1\n",
    "train_real_pred['fp']=result1['forward_count']\n",
    "train_real_pred['cp']=result1['comment_count']\n",
    "train_real_pred['lp']=result1['like_count']\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (Factors: Media, Length, Emoji, Median,Polarity) with OLS\n",
    "\n",
    "### OLS is a type of linear least sqaures methods for estimating parameters in a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1=train[[\"content_media_count\",\"content_length\",\"forward_median\",\"comment_median\",\"like_median\",\"polarity\"]]\n",
    "Y_train1=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test1=predict[[\"content_media_count\",\"content_length\",\"forward_median\",\"comment_median\",\"like_median\",\"polarity\"]]\n",
    "Y_test1=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train1.fillna(X_train1.max(),inplace=True)\n",
    "X_test1.fillna(X_test1.max(),inplace=True)\n",
    "model1=sm.OLS(Y_train1,X_train1).fit()\n",
    "pred1=model1.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "np.savetxt(\"C:/Users/user/Downloads/weibo_predict_result52.csv\",pred1,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result1=pd.read_csv(\"C:/Users/user/Downloads/weibo_predict_result52.csv\")\n",
    "\n",
    "train_real_pred=Y_test1\n",
    "train_real_pred['fp']=result1['forward_count']\n",
    "train_real_pred['cp']=result1['comment_count']\n",
    "train_real_pred['lp']=result1['like_count']\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model : (Factors: Media, Length, Emoji, Median,Polarity) with Ridge regression\n",
    "\n",
    "### Ridge regression is used to prevent multicollinearity among variables by shrinking the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1=train[[\"content_media_count\",\"content_length\",\"forward_median\",\"comment_median\",\"like_median\",\"polarity\"]]\n",
    "Y_train1=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test1=predict[[\"content_media_count\",\"content_length\",\"forward_median\",\"comment_median\",\"like_median\",\"polarity\"]]\n",
    "Y_test1=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train1.fillna(X_train1.max(),inplace=True)\n",
    "X_test1.fillna(X_test1.max(),inplace=True)\n",
    "lm1=linear_model.Ridge(alpha=3)\n",
    "model1=lm1.fit(X_train1,Y_train1)\n",
    "pred1=lm1.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "np.savetxt(\"C:/Users/user/Downloads/weibo_predict_result53.csv\",pred1,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result1=pd.read_csv(\"C:/Users/user/Downloads/weibo_predict_result53.csv\")\n",
    "train_real_pred=Y_test1\n",
    "train_real_pred['fp']=result1['forward_count']\n",
    "train_real_pred['cp']=result1['comment_count']\n",
    "train_real_pred['lp']=result1['like_count']\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (Factors: Media, Length, Emoji, Median,Polarity) with Lasso regression\n",
    "\n",
    "### Lasso regression does automatic feature selection that means if some features are correlated then lasso will pick only one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1=train[[\"content_media_count\",\"content_length\",\"forward_median\",\"comment_median\",\"like_median\",\"polarity\"]]\n",
    "Y_train1=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test1=predict[[\"content_media_count\",\"content_length\",\"forward_median\",\"comment_median\",\"like_median\",\"polarity\"]]\n",
    "Y_test1=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train1.fillna(X_train1.max(),inplace=True)\n",
    "X_test1.fillna(X_test1.max(),inplace=True)\n",
    "lm1=Lasso(alpha=0.01)\n",
    "model1=lm1.fit(X_train1,Y_train1)\n",
    "pred1=lm1.predict(X_test1)\n",
    "pred1=pred1.round()\n",
    "pred1=(np.maximum(pred1,0.))\n",
    "np.savetxt(\"C:/Users/user/Downloads/weibo_predict_result54.csv\",pred1,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result1=pd.read_csv(\"C:/Users/user/Downloads/weibo_predict_result54.csv\")\n",
    "np.savetxt(\"C:/Users/user/Downloads/weibo_predict_result54.csv\",pred1,delimiter=',',header=\"forward_count,comment_count,like_count\",comments=\"\")\n",
    "result1=pd.read_csv(\"C:/Users/user/Downloads/weibo_predict_result54.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling with Normalized Polarity\n",
    "Contribution: Apoorva Malemath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        \n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1=pd.read_csv(\"E://DMA_PRED//polarityL1.csv\")\n",
    "\n",
    "max_value = df1['polarity'].max()\n",
    "min_value = df1['polarity'].min()\n",
    "df1['pnorm'] = (df1['polarity'] - min_value) / (max_value - min_value)\n",
    "df=df1.drop(['Unnamed: 0'], axis=1)\n",
    "df = sklearn.utils.shuffle(df)\n",
    "uid_stat=pd.read_csv(\"E:\\\\DMA_PRE\\\\train_uid_stat.csv\")\n",
    "df = pd.merge(df,uid_stat , on=['u_id'])\n",
    "\n",
    "df.columns\n",
    "\n",
    "train=df[0:80000]\n",
    "predict=df[80001:100000]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "features_train=train[['content_media_count','pnorm','forward_min', 'forward_max', 'forward_median', 'forward_mean',\n",
    "       'comment_min', 'comment_max', 'comment_median', 'comment_mean',\n",
    "       'like_min', 'like_max', 'like_median', 'like_mean']]\n",
    "features_test=predict[['content_media_count','pnorm','forward_min', 'forward_max', 'forward_median', 'forward_mean',\n",
    "       'comment_min', 'comment_max', 'comment_median', 'comment_mean',\n",
    "       'like_min', 'like_max', 'like_median', 'like_mean']]\n",
    "labels_train=train[['forward_count', 'comment_count', 'like_count']]\n",
    "labels_test=predict[['forward_count', 'comment_count', 'like_count']]\n",
    "\n",
    "x = features_train\n",
    "y = labels_train\n",
    "x1 = features_test\n",
    "y1 = labels_test\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=50, random_state=0,n_estimators=100)\n",
    "regr.fit(x, y)\n",
    "pred2 = regr.predict(x1)\n",
    "temp = pd.DataFrame.from_records(pred2)\n",
    "temp=temp.round()\n",
    "temp=(np.maximum(temp,0))\n",
    "temp=temp.abs()\n",
    "temp=temp.astype(int)\n",
    "\n",
    "train_real_pred=y1\n",
    "train_real_pred['fp']=temp[0].values\n",
    "train_real_pred['cp']=temp[1].values\n",
    "train_real_pred['lp']=temp[2].values\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(x,y)\n",
    "pred1=lm.predict(x1)\n",
    "\n",
    "temp = pd.DataFrame.from_records(pred1)\n",
    "temp=temp.round()\n",
    "temp=(np.maximum(temp,0))\n",
    "train_real_pred=y1\n",
    "train_real_pred['fp']=temp[0].values\n",
    "train_real_pred['cp']=temp[1].values\n",
    "train_real_pred['lp']=temp[2].values\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble - Averaging\n",
    "Contribution: Apoorva Malemath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"E:\\\\5th-Sem\\\\DMA Project\\\\Project\\\\weibo_train1_cpts.csv\")\n",
    "df2=pd.read_csv(\"E:\\\\5th-Sem\\\\DMA Project\\\\Project\\\\weibo_train2_cpts.csv\")\n",
    "frames=[df1,df2]\n",
    "train_all=pd.concat(frames)\n",
    "train=train_all[0:983694]\n",
    "predict=train_all[983695:1229618]\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"content_media_count\",\"content_length\",\"content_emoji_count\",\"hour\",\"min\",\"sec\",\"forward_median\",\"comment_median\",\"like_median\"]]\n",
    "Y_train=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test=predict[[\"content_media_count\",\"content_length\",\"content_emoji_count\",\"hour\",\"min\",\"sec\",\"forward_median\",\"comment_median\",\"like_median\"]]\n",
    "Y_test=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred1=lm.predict(X_test)\n",
    "temp = pd.DataFrame.from_records(pred1)\n",
    "temp=temp.round()\n",
    "temp=(np.maximum(temp,0))\n",
    "train_real_pred=Y_test\n",
    "train_real_pred['fp']=temp[0].values\n",
    "train_real_pred['cp']=temp[1].values\n",
    "train_real_pred['lp']=temp[2].values\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spliting of training dataset into 70% training data and 30% testing data randomly\n",
    "features_train=train[[\"content_media_count\",\"content_#_count\",\"content_length\",\"content_emoji_count\",\"forward_median\",\"comment_median\",\"like_median\"]]\n",
    "features_test=predict[[\"content_media_count\",\"content_#_count\",\"content_length\",\"content_emoji_count\",\"forward_median\",\"comment_median\",\"like_median\"]]\n",
    "labels_train=train[['forward_count', 'comment_count', 'like_count']]\n",
    "labels_test=predict[['forward_count', 'comment_count', 'like_count']]\n",
    "\n",
    "x = features_train\n",
    "y = labels_train\n",
    "x1 = features_test\n",
    "y1 = labels_test\n",
    "\n",
    "\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=50, random_state=0,n_estimators=100)\n",
    "regr.fit(x, y)\n",
    "pred2 = regr.predict(x1)\n",
    "temp = pd.DataFrame.from_records(pred2)\n",
    "temp=temp.round()\n",
    "temp=(np.maximum(temp,0))\n",
    "temp=temp.abs()\n",
    "temp=temp.astype(int)\n",
    "train_real_pred=Y_test\n",
    "train_real_pred['fp']=temp[0].values\n",
    "train_real_pred['cp']=temp[1].values\n",
    "train_real_pred['lp']=temp[2].values\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=sm.OLS(Y_train,X_train).fit()\n",
    "pred3=model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4- Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1=linear_model.Ridge(alpha=3)\n",
    "model4=lm1.fit(X_train,Y_train)\n",
    "pred4=lm1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5- Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1=Lasso(alpha=0.01)\n",
    "model5=lm1.fit(X_train,Y_train)\n",
    "pred5=lm1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble - Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=(pred1+pred2+pred3+pred4+pred5)/5\n",
    "pred=pred.round()\n",
    "pred=(np.maximum(pred,0))\n",
    "pred=pred.abs()\n",
    "pred1=pred.astype(int)\n",
    "train_real_pred=Y_test\n",
    "train_real_pred['fp']=pred1[0]\n",
    "train_real_pred['cp']=pred1[1]\n",
    "train_real_pred['lp']=pred1[2]\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping UID \n",
    "Coontribution: Apoorva Malemath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id=train_all['u_id'].unique().tolist()\n",
    "uid_df = pd.DataFrame({'u_id':unique_id})\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "l=le.transform(unique_id) \n",
    "df = pd.DataFrame({'u_id':l})\n",
    "uid_df['id']=df['u_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all=train_all.set_index('u_id').join(uid_df.set_index('u_id'))\n",
    "train=train_all[0:983694]\n",
    "predict=train_all[983695:1229618]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[[\"id\",\"content_media_count\",\"content_length\",\"content_emoji_count\",\"hour\",\"min\",\"sec\",\"forward_median\",\"comment_median\",\"like_median\"]]\n",
    "Y_train=train[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "X_test=predict[[\"id\",\"content_media_count\",\"content_length\",\"content_emoji_count\",\"hour\",\"min\",\"sec\",\"fo\n",
    "                rward_median\",\"comment_median\",\"like_median\"]]\n",
    "Y_test=predict[[\"forward_count\",\"comment_count\",\"like_count\"]]\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "X_train.fillna(X_train.max(),inplace=True)\n",
    "X_test.fillna(X_test.max(),inplace=True)\n",
    "\n",
    "lm=linear_model.LinearRegression()\n",
    "model=lm.fit(X_train,Y_train)\n",
    "pred1=lm.predict(X_test)\n",
    "temp = pd.DataFrame.from_records(pred1)\n",
    "temp=temp.round()\n",
    "temp=(np.maximum(temp,0))\n",
    "temp=temp.abs()\n",
    "temp=temp.astype(int)\n",
    "train_real_pred=Y_test\n",
    "train_real_pred['fp']=temp[0].values\n",
    "train_real_pred['cp']=temp[1].values\n",
    "train_real_pred['lp']=temp[2].values\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spliting of training dataset into 70% training data and 30% testing data randomly\n",
    "features_train=train[[\"id\",\"content_media_count\",\"content_#_count\",\"content_length\",\"content_emoji_count\",\"forward_median\",\"comment_median\",\"like_median\"]]\n",
    "features_test=predict[[\"id\",\"content_media_count\",\"content_#_count\",\"content_length\",\"content_emoji_count\",\"forward_median\",\"comment_median\",\"like_median\"]]\n",
    "labels_train=train[['forward_count', 'comment_count', 'like_count']]\n",
    "labels_test=predict[['forward_count', 'comment_count', 'like_count']]\n",
    "\n",
    "x = features_train\n",
    "y = labels_train\n",
    "x1 = features_test\n",
    "y1 = labels_test\n",
    "\n",
    "\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=50, random_state=0,n_estimators=100)\n",
    "regr.fit(x, y)\n",
    "pred2 = regr.predict(x1)\n",
    "temp = pd.DataFrame.from_records(pred2)\n",
    "temp=temp.round()\n",
    "temp=(np.maximum(temp,0))\n",
    "temp=temp.abs()\n",
    "temp=temp.astype(int)\n",
    "train_real_pred=Y_test\n",
    "train_real_pred['fp']=temp[0].values\n",
    "train_real_pred['cp']=temp[1].values\n",
    "train_real_pred['lp']=temp[2].values\n",
    "print(\"Score:{0:.2f}%\".format(precision(train_real_pred.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best F C L Match Model\n",
    "Contribution: Ashish Kar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from genUidStat import loadData,genUidStat\n",
    "from evaluation import precision\n",
    "from runTime import runTime\n",
    "from pathos.pools import _ProcessPool\n",
    "from multiprocess.pool import Pool\n",
    "\n",
    "df1=pd.read_csv(\"weibo_train1.csv\")\n",
    "df2=pd.read_csv(\"weibo_train2.csv\")\n",
    "frames=[df1,df2]\n",
    "traindata=pd.concat(frames)\n",
    "\n",
    "def splitDataFrameIntoSmaller(df, chunkSize = 10000): \n",
    "    listOfDf = list()\n",
    "    numberChunks = len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n",
    "    return listOfDf\n",
    "\n",
    "uid_stat=pd.read_csv(\"train_uid_stat.csv\")\n",
    "\n",
    "uid = splitDataFrameIntoSmaller(uid_stat, chunkSize = 500)\n",
    "\n",
    "uid[0].shape[0]\n",
    "\n",
    "# Generation of Best Scoring F C L for each UID\n",
    "\n",
    "def search_all_uid(stat_dic,file):\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\tdef _deviation(predict, real, kind):\n",
    "\t\tt = 5.0 if kind=='f' else 3.0\n",
    "\t\treturn abs(predict - real) / (real + t)\n",
    "\tdef _precision_i(fp, fr, cp, cr, lp, lr):\n",
    "\t\treturn 1 - 0.5 * _deviation(fp, fr, 'f') - 0.25 * _deviation(cp, cr, 'c') - 0.25 * _deviation(lp, lr, 'l')\n",
    "\tdef _sgn(x):\n",
    "\t\treturn 1 if x>0 else 0\n",
    "\tdef _count_i(fr, cr, lr):\n",
    "\t\tx = fr + cr + lr\n",
    "\t\treturn 101 if x>100 else (x+1)\n",
    "\tdef precision(real_and_predict):\n",
    "\t\tnumerator,denominator = 0.0,0.0\n",
    "\t\tfor  fr, cr, lr,fp, cp, lp in real_and_predict:\n",
    "\t\t\tnumerator += _count_i(fr, cr, lr) * _sgn(_precision_i(fp, fr, cp, cr, lp, lr) - 0.8)\n",
    "\t\t\tdenominator += _count_i(fr, cr, lr)\n",
    "\t\treturn (numerator / denominator)\n",
    "\tdef score(uid_data,pred):\n",
    "\t\t\"\"\"\n",
    "\t\tuid_data:\n",
    "\t\t\tpd.DataFrame\n",
    "\t\tpred:\n",
    "\t\t\tlist, [fp,cp,lp]\n",
    "\t\t\"\"\"\n",
    "\t\tuid_real_pred = uid_data[['forward_count','comment_count','like_count']]\n",
    "\t\tuid_real_pred['fp'] = pred[0]\n",
    "\t\tuid_real_pred['cp'] = pred[1]\n",
    "\t\tuid_real_pred['lp'] = pred[2]\n",
    "\t\treturn precision(uid_real_pred.values)\n",
    "\tdef search(uid_data,target,args):\n",
    "\t\targs = list(args)\n",
    "\t\ttarget_index = ['forward_count','comment_count','like_count'].index(target)\n",
    "\t\ttarget_min,target_median,target_max = args[3*target_index:3*target_index+3]\n",
    "\t\tdel args[3*target_index:3*target_index+3]\n",
    "\t\tpred = (args[1],args[4])\n",
    "\t\t\n",
    "\t\tbest_num = [target_median]\n",
    "\t\tbest_pred = list(pred)\n",
    "\t\tbest_pred.insert(target_index,target_median)\n",
    "\t\tbest_score = score(uid_data,best_pred)\n",
    "\t\tfor num in range(target_min,target_max+1):\n",
    "\t\t\tthis_pred = list(pred)\n",
    "\t\t\tthis_pred.insert(target_index,num)\n",
    "\t\t\tthis_score = score(uid_data,this_pred)\n",
    "\t\t\tif this_score >= best_score:                  \n",
    "\t\t\t\tif this_score > best_score:\n",
    "\t\t\t\t\tbest_num = [num]\n",
    "\t\t\t\t\tbest_score = this_score\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbest_num.append(num)                       \n",
    "\t\t\t\t\n",
    "\t\treturn best_num[np.array([abs(i - target_median) for i in best_num]).argmin()]\n",
    "\tuid_best_pred = {}\n",
    "\tpool = _ProcessPool()\n",
    "\tuids,f,c,l = [],[],[],[]\n",
    "\tm=1\n",
    "\tfor uid in stat_dic:\n",
    "\t\tprint (\"search uid:{}\".format(uid),m)\n",
    "\t\tm=m+1\n",
    "\t\tuid_data = traindata[traindata.u_id == uid]\n",
    "\t\targuments = stat_dic[uid][['forward_min','forward_median','forward_max','comment_min',\\\n",
    "\t\t\t\t\t'comment_median','comment_max','like_min','like_median','like_max']]\n",
    "\t\targuments = tuple([int(i) for i in arguments]) \n",
    "\t\tf.append(pool.apply_async(search,args=(uid_data,'forward_count',arguments)))\n",
    "\t\tc.append(pool.apply_async(search,args=(uid_data,'comment_count',arguments)))\n",
    "\t\tl.append(pool.apply_async(search,args=(uid_data,'like_count',arguments)))\n",
    "\t\tuids.append(uid)\n",
    "\tpool.close()\n",
    "\tpool.join()\n",
    "\tf = [i.get() for i in f]\n",
    "\tc = [i.get() for i in c]\n",
    "\tl = [i.get() for i in l]\n",
    "\tfor i in range(len(uids)):\n",
    "\t\tuid_best_pred[uids[i]] = [f[i],c[i],l[i]]\n",
    "\t#cPickle.dump(uid_best_pred,open('uid_best_pred'+str(file)+'.pkl','ab'))\n",
    "\tlabel = ['forward_count','comment_count','like_count']\n",
    "\tpd.DataFrame.from_dict(data=uid_best_pred,orient='index').to_csv(\"G:\\\\Anconda Prog\\\\BestPred\\\\weibo_uidbest\"+str(file)+\".csv\",header=label)\n",
    "\tprint(\"Written to file\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uid_stat=pd.read_csv(\"train_uid_stat.csv\")\n",
    "uid_stat=uid_stat.set_index('u_id')\n",
    "uid = splitDataFrameIntoSmaller(uid_stat, chunkSize = 100)\n",
    "n=368\n",
    "while n<373:\n",
    "    df=uid[n].T\n",
    "    stat=df.to_dict('series')\n",
    "    n=n+1\n",
    "    search_all_uid(stat,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=0\n",
    "for i in range(0,373):\n",
    "\tst[i]=pd.read_csv(\"weibo_uidbest\"+str(i)+\".csv\")\n",
    "for i in range(0,373):\n",
    "\tstring=string+\"st[\"+str(i)+\"],\"\n",
    "string=string[:-1]\n",
    "frames=[string]\n",
    "uid_best_pred=pd.concat(frames)\n",
    "uid_best_pred.rename(columns={uid_best_pred[0]:\"u_id\"})\n",
    "uid_best_pred.to_csv(\"train_best_pred.csv\",sep=',',index=False,encoding='utf-8')\n",
    "\n",
    "# FILES GENERATED..... NOW RESULT GENERATION\n",
    "\n",
    "@runTime\n",
    "def predict_by_search(submission=True):\n",
    "\ttraindata,testdata = loadData()\n",
    "\tub=pd.read_csv(\"train_best_pred.csv\")\n",
    "\tub=ub.set_index('u_id')\n",
    "\tdf=ub.T\n",
    "\tuid_best_pred=df.to_dict('series')\n",
    "\t#uid_best_pred = search_all_uid()\n",
    "\t#print (\"search done,now predict on traindata and testdata...\")\n",
    "\n",
    "\t#predict traindata with uid's best fp,cp,lp\n",
    "\tforward,comment,like = [],[],[]\n",
    "\tfor uid in traindata['u_id']:\n",
    "\t\tif uid in uid_best_pred:\n",
    "\t\t\tforward.append(int(uid_best_pred[uid][0]))\n",
    "\t\t\tcomment.append(int(uid_best_pred[uid][1]))\n",
    "\t\t\tlike.append(int(uid_best_pred[uid][2]))\n",
    "\t\telse:\n",
    "\t\t\tforward.append(0)\n",
    "\t\t\tcomment.append(0)\n",
    "\t\t\tlike.append(0)\n",
    "\t#score on the traindata\n",
    "\ttrain_real_pred = traindata[['forward_count','comment_count','like_count']]\n",
    "\ttrain_real_pred['fp'],train_real_pred['cp'],train_real_pred['lp'] = forward,comment,like\n",
    "\tprint (\"Score on the training set:{0:.2f}%\".format(precision(train_real_pred.values)*100))\n",
    "\tif submission:\n",
    "\t\ttest_pred = testdata[['u_id','m_id']]\n",
    "\t\tforward,comment,like = [],[],[]\n",
    "\t\tfor uid in testdata['u_id']:\n",
    "\t\t\tif uid in uid_best_pred:\n",
    "\t\t\t\tforward.append(int(uid_best_pred[uid][0]))\n",
    "\t\t\t\tcomment.append(int(uid_best_pred[uid][1]))\n",
    "\t\t\t\tlike.append(int(uid_best_pred[uid][2]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tforward.append(0)\n",
    "\t\t\t\tcomment.append(0)\n",
    "\t\t\t\tlike.append(0)\n",
    "\t\ttest_pred['fp'],test_pred['cp'],test_pred['lp'] = forward,comment,like\n",
    "\t\t\n",
    "\t\t#generate submission file\n",
    "\t\tresult = []\n",
    "\t\tfilename = \"weibo_predict_search.txt\"\n",
    "\t\tfor _,row in test_pred.iterrows():\n",
    "\t\t\tresult.append(\"{0}\\t{1}\\t{2},{3},{4}\\n\".format(row[0],row[1],row[2],row[3],row[4]))\n",
    "\t\tf = open(filename,'w')\n",
    "\t\tf.writelines(result)\n",
    "\t\tf.close()\n",
    "\t\tprint ('generate submission file \"{}\"'.format(filename))\n",
    "\n",
    "predict_by_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------Performance Analysis and Lederboard Ranking-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard Ranked Model Analysis \n",
    "Contribution: All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Best Model Evaluation\n",
    "\n",
    "## Model 1: UID FCL Median Model\n",
    "## Train Score: 32.73 Official Score:29.38989214 Rank:241\n",
    "\n",
    "df3=pd.read_csv(\"weibo_train1_cpts.csv\")\n",
    "df4=pd.read_csv(\"weibo_train2_cpts.csv\")\n",
    "frames=[df3,df4]\n",
    "train_dataset=pd.concat(frames)\n",
    "\n",
    "print(mean_squared_error(train_dataset['forward_count'],train_dataset['forward_median']))\n",
    "print(mean_squared_error(train_dataset['comment_count'],train_dataset['comment_median']))\n",
    "print(mean_squared_error(train_dataset['like_count'],train_dataset['like_median']))\n",
    "\n",
    "print(accuracy_score(train_dataset['forward_count'],train_dataset['forward_median']))\n",
    "print(accuracy_score(train_dataset['comment_count'],train_dataset['comment_median']))\n",
    "print(accuracy_score(train_dataset['like_count'],train_dataset['like_median']))\n",
    "\n",
    "## Model 2: UID FCL Best Match Model\n",
    "## Train Score: 34.76 Official Score:29.67695154 (29.68) Rank:218\n",
    "\n",
    "df1=pd.read_csv(\"weibo_train1.csv\")\n",
    "df2=pd.read_csv(\"weibo_train2.csv\")\n",
    "frames=[df1,df2]\n",
    "train_dataset=pd.concat(frames)\n",
    "\n",
    "stat=pd.read_csv(\"train_best_pred.csv\")\n",
    "\n",
    "trainstat=pd.merge(train_dataset,stat,on=['u_id'],how='left')\n",
    "\n",
    "trainstat.shape\n",
    "\n",
    "trainstat.head(614809).to_csv(\"weibo_train1_final.csv\",sep=',',index=False,encoding='utf-8')\n",
    "trainstat.tail(614809).to_csv(\"weibo_train2_final.csv\",sep=',',index=False,encoding='utf-8')\n",
    "\n",
    "df3=pd.read_csv(\"weibo_train1_final.csv\")\n",
    "df4=pd.read_csv(\"weibo_train2_final.csv\")\n",
    "frames=[df3,df4]\n",
    "train_dataset=pd.concat(frames)\n",
    "\n",
    "\n",
    "print(mean_squared_error(train_dataset['forward_count_x'],train_dataset['forward_count_y']))\n",
    "print(mean_squared_error(train_dataset['comment_count_x'],train_dataset['comment_count_y']))\n",
    "print(mean_squared_error(train_dataset['like_count_x'],train_dataset['like_count_y']))\n",
    "\n",
    "print(accuracy_score(train_dataset['forward_count_x'],train_dataset['forward_count_y']))\n",
    "print(accuracy_score(train_dataset['comment_count_x'],train_dataset['comment_count_y']))\n",
    "print(accuracy_score(train_dataset['like_count_x'],train_dataset['like_count_y']))\n",
    "\n",
    "## Summary\n",
    "\n",
    "![](fcomp.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------Conclusions---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built models to predict the number of forwards, comments and likes for a weibo. We followed an iterative KDD process to refine the pre-processing of data and improve the performance of model. As the crucial part of our data is text, we apply text preprocessing techniques namely removal of noise, stop words, stemming and lemmatization.\n",
    "\n",
    "We found that using the algorithmic approach where we defined the best possible value for each user based the UID yielded good result. The model used statistical factors and yielded a score of 29.82% and got a rank of 218 on leaderboard.\n",
    "\n",
    "Also computing polarity of the text and performing a min max normalization on it yielded a score of 78.92% on the cross validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
